{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 4-3. TensorFlow Implementation of Reccurent Neural Network using LSTM for Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 1,step:50, Minibatch Loss= 0.874432, Training Accuracy= 0.67188\n",
      "Epoch: 1,step:100, Minibatch Loss= 0.837254, Training Accuracy= 0.77344\n",
      "Epoch: 1,step:150, Minibatch Loss= 0.261696, Training Accuracy= 0.94531\n",
      "Epoch: 1,step:200, Minibatch Loss= 0.406846, Training Accuracy= 0.88281\n",
      "Epoch: 1,step:250, Minibatch Loss= 0.227278, Training Accuracy= 0.93750\n",
      "Epoch: 1,step:300, Minibatch Loss= 0.159254, Training Accuracy= 0.92969\n",
      "Epoch: 1,step:350, Minibatch Loss= 0.260627, Training Accuracy= 0.91406\n",
      "Epoch: 1,step:400, Minibatch Loss= 0.130052, Training Accuracy= 0.94531\n",
      "Epoch: 2,step:50, Minibatch Loss= 0.252877, Training Accuracy= 0.92188\n",
      "Epoch: 2,step:100, Minibatch Loss= 0.096116, Training Accuracy= 0.97656\n",
      "Epoch: 2,step:150, Minibatch Loss= 0.099564, Training Accuracy= 0.97656\n",
      "Epoch: 2,step:200, Minibatch Loss= 0.123245, Training Accuracy= 0.96875\n",
      "Epoch: 2,step:250, Minibatch Loss= 0.108197, Training Accuracy= 0.96094\n",
      "Epoch: 2,step:300, Minibatch Loss= 0.050546, Training Accuracy= 0.99219\n",
      "Epoch: 2,step:350, Minibatch Loss= 0.178440, Training Accuracy= 0.93750\n",
      "Epoch: 2,step:400, Minibatch Loss= 0.107138, Training Accuracy= 0.96094\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.972656\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "#Import the Required Libraries\n",
    "##################################\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "epochs = 2\n",
    "num_train = mnist.train.num_examples \n",
    "num_batches = (num_train//batch_size) + 1\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match LSTM  \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    \n",
    "    while i < epochs:\n",
    "        for step in xrange(num_batches):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            if (step + 1) % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "                # Calculate batch loss\n",
    "                loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "                print \"Epoch: \" + str(i+1) + \",step:\"+ str(step+1) +\", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc)\n",
    "        i += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 1024\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 4-4. Next word Prediction and sentence completion in TensorFlow using Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "Iter= 500, Average Loss= 4.242353, Average Accuracy= 16.60%\n",
      "['so', 'stingy', 'about'] - Actual word:[it] vs Predicted word:[it]\n",
      "Iter= 1000, Average Loss= 3.598423, Average Accuracy= 29.20%\n",
      "['.', 'however,', 'she'] - Actual word:[did] vs Predicted word:[she]\n",
      "Iter= 1500, Average Loss= 2.073462, Average Accuracy= 46.20%\n",
      "['barley-sugar', 'and', 'such'] - Actual word:[things] vs Predicted word:[things]\n",
      "Iter= 2000, Average Loss= 0.798790, Average Accuracy= 77.80%\n",
      "[',', 'because', 'she'] - Actual word:[was] vs Predicted word:[was]\n",
      "Iter= 2500, Average Loss= 0.338358, Average Accuracy= 91.60%\n",
      "[',', \"'\", 'and'] - Actual word:[vinegar] vs Predicted word:[won't]\n",
      "Iter= 3000, Average Loss= 0.366451, Average Accuracy= 90.40%\n",
      "['so', 'close', 'to'] - Actual word:[her] vs Predicted word:[her]\n",
      "Iter= 3500, Average Loss= 1.110132, Average Accuracy= 74.20%\n",
      "['hot-tempered', ',', \"'\"] - Actual word:[she] vs Predicted word:[she]\n",
      "Iter= 4000, Average Loss= 0.522139, Average Accuracy= 88.00%\n",
      "['squeezed', 'herself', 'up'] - Actual word:[closer] vs Predicted word:[closer]\n",
      "Iter= 4500, Average Loss= 0.299359, Average Accuracy= 93.40%\n",
      "[\"'\", 'i', \"won't\"] - Actual word:[have] vs Predicted word:[have]\n",
      "Iter= 5000, Average Loss= 0.527236, Average Accuracy= 89.60%\n",
      "['the', 'duchess', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 5500, Average Loss= 0.191242, Average Accuracy= 97.00%\n",
      "['to', 'herself', ','] - Actual word:[(] vs Predicted word:[(]\n",
      "Iter= 6000, Average Loss= 0.215474, Average Accuracy= 95.80%\n",
      "[',', \"'\", 'alice'] - Actual word:[ventured] vs Predicted word:[ventured]\n",
      "Iter= 6500, Average Loss= 0.096967, Average Accuracy= 98.60%\n",
      "['savage', 'when', 'they'] - Actual word:[met] vs Predicted word:[met]\n",
      "Iter= 7000, Average Loss= 0.312939, Average Accuracy= 94.00%\n",
      "['but', 'i', 'shall'] - Actual word:[remember] vs Predicted word:[remember]\n",
      "Iter= 7500, Average Loss= 0.085992, Average Accuracy= 98.80%\n",
      "['herself', 'that', 'perhaps'] - Actual word:[it] vs Predicted word:[it]\n",
      "Iter= 8000, Average Loss= 0.191885, Average Accuracy= 97.40%\n",
      "['dear', ',', 'and'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 8500, Average Loss= 0.163213, Average Accuracy= 97.60%\n",
      "['affectionately', 'into', \"alice's\"] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 9000, Average Loss= 0.118021, Average Accuracy= 98.40%\n",
      "['the', 'duchess', 'by'] - Actual word:[this] vs Predicted word:[this]\n",
      "Iter= 9500, Average Loss= 0.125797, Average Accuracy= 98.40%\n",
      "['think', 'how', 'glad'] - Actual word:[i] vs Predicted word:[i]\n",
      "Iter= 10000, Average Loss= 0.052558, Average Accuracy= 99.20%\n",
      "['then', 'they', \"wouldn't\"] - Actual word:[be] vs Predicted word:[be]\n",
      "Iter= 10500, Average Loss= 0.189518, Average Accuracy= 97.60%\n",
      "['chin', '.', 'however,'] - Actual word:[she] vs Predicted word:[shoulder]\n",
      "Iter= 11000, Average Loss= 0.059204, Average Accuracy= 99.20%\n",
      "['sour', '\\xe2\\x80\\x94', 'and'] - Actual word:[camomile] vs Predicted word:[camomile]\n",
      "Iter= 11500, Average Loss= 0.052956, Average Accuracy= 99.00%\n",
      "['ugly', ';', 'and'] - Actual word:[secondly] vs Predicted word:[secondly]\n",
      "Iter= 12000, Average Loss= 0.178871, Average Accuracy= 98.00%\n",
      "['having', 'found', 'out'] - Actual word:[a] vs Predicted word:[a]\n",
      "Iter= 12500, Average Loss= 0.128222, Average Accuracy= 98.20%\n",
      "['did', 'not', 'much'] - Actual word:[like] vs Predicted word:[like]\n",
      "Iter= 13000, Average Loss= 0.080045, Average Accuracy= 98.60%\n",
      "['soup', 'does', 'very'] - Actual word:[well] vs Predicted word:[well]\n",
      "Iter= 13500, Average Loss= 0.131601, Average Accuracy= 98.00%\n",
      "['tut', ',', 'child'] - Actual word:[!] vs Predicted word:[!]\n",
      "Iter= 14000, Average Loss= 0.144819, Average Accuracy= 97.20%\n",
      "['the', 'kitchen', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 14500, Average Loss= 0.135645, Average Accuracy= 97.40%\n",
      "['i', 'shall', 'remember'] - Actual word:[it] vs Predicted word:[it]\n",
      "Iter= 15000, Average Loss= 0.046482, Average Accuracy= 99.20%\n",
      "['temper', ',', 'and'] - Actual word:[thought] vs Predicted word:[thought]\n",
      "Iter= 15500, Average Loss= 0.026436, Average Accuracy= 99.60%\n",
      "['dear', ',', 'and'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 16000, Average Loss= 0.162256, Average Accuracy= 97.80%\n",
      "['and', 'they', 'walked'] - Actual word:[off] vs Predicted word:[off]\n",
      "Iter= 16500, Average Loss= 0.051712, Average Accuracy= 98.80%\n",
      "['her', 'voice', 'close'] - Actual word:[to] vs Predicted word:[to]\n",
      "Iter= 17000, Average Loss= 0.178047, Average Accuracy= 97.00%\n",
      "['again', ',', 'you'] - Actual word:[dear] vs Predicted word:[dear]\n",
      "Iter= 17500, Average Loss= 0.189837, Average Accuracy= 97.60%\n",
      "[\"'she\", 'had', 'quite'] - Actual word:[forgotten] vs Predicted word:[forgotten]\n",
      "Iter= 18000, Average Loss= 0.107726, Average Accuracy= 98.40%\n",
      "['not', 'like', 'to'] - Actual word:[be] vs Predicted word:[be]\n",
      "Iter= 18500, Average Loss= 0.104130, Average Accuracy= 98.20%\n",
      "['\\xe2\\x80\\x94', 'and', '\\xe2\\x80\\x94'] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 19000, Average Loss= 0.086713, Average Accuracy= 98.60%\n",
      "['secondly', ',', 'because'] - Actual word:[she] vs Predicted word:[she]\n",
      "Iter= 19500, Average Loss= 0.145749, Average Accuracy= 98.60%\n",
      "['pleased', 'at', 'having'] - Actual word:[found] vs Predicted word:[found]\n",
      "Iter= 20000, Average Loss= 0.061419, Average Accuracy= 98.60%\n",
      "['spoke', '.', 'alice'] - Actual word:[did] vs Predicted word:[did]\n",
      "Iter= 20500, Average Loss= 0.071120, Average Accuracy= 99.00%\n",
      "['pepper', 'in', 'my'] - Actual word:[kitchen] vs Predicted word:[kitchen]\n",
      "Iter= 21000, Average Loss= 0.086258, Average Accuracy= 98.80%\n",
      "[\"'\", \"everything's\", 'got'] - Actual word:[a] vs Predicted word:[a]\n",
      "Iter= 21500, Average Loss= 0.081471, Average Accuracy= 99.00%\n",
      "['to', 'herself', ','] - Actual word:[(] vs Predicted word:[(]\n",
      "Iter= 22000, Average Loss= 0.042899, Average Accuracy= 99.20%\n",
      "['remark', '.', \"'\"] - Actual word:[tut] vs Predicted word:[tut]\n",
      "Iter= 22500, Average Loss= 0.067484, Average Accuracy= 98.80%\n",
      "['the', 'kitchen', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 23000, Average Loss= 0.092514, Average Accuracy= 99.00%\n",
      "['moral', 'of', 'that'] - Actual word:[is] vs Predicted word:[is]\n",
      "Iter= 23500, Average Loss= 0.132825, Average Accuracy= 98.40%\n",
      "['such', 'a', 'pleasant'] - Actual word:[temper] vs Predicted word:[temper]\n",
      "Iter= 24000, Average Loss= 0.076569, Average Accuracy= 98.20%\n",
      "['my', 'dear', ','] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 24500, Average Loss= 0.092625, Average Accuracy= 98.40%\n",
      "['she', 'tucked', 'her'] - Actual word:[arm] vs Predicted word:[arm]\n",
      "Iter= 25000, Average Loss= 0.003911, Average Accuracy= 99.60%\n",
      "['she', 'heard', 'her'] - Actual word:[voice] vs Predicted word:[voice]\n",
      "Iter= 25500, Average Loss= 0.041589, Average Accuracy= 99.60%\n",
      "[',', 'you', 'dear'] - Actual word:[old] vs Predicted word:[old]\n",
      "Iter= 26000, Average Loss= 0.110294, Average Accuracy= 98.60%\n",
      "['stingy', 'about', 'it'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 26500, Average Loss= 0.093003, Average Accuracy= 98.60%\n",
      "['however,', 'she', 'did'] - Actual word:[not] vs Predicted word:[not]\n",
      "Iter= 27000, Average Loss= 0.061559, Average Accuracy= 99.00%\n",
      "['that', 'make', 'children'] - Actual word:[sweet-tempered] vs Predicted word:[sweet-tempered]\n",
      "Iter= 27500, Average Loss= 0.036433, Average Accuracy= 99.60%\n",
      "['upon', \"alice's\", 'shoulder'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 28000, Average Loss= 0.058808, Average Accuracy= 99.00%\n",
      "['vinegar', 'that', 'makes'] - Actual word:[them] vs Predicted word:[them]\n",
      "Iter= 28500, Average Loss= 0.133447, Average Accuracy= 97.40%\n",
      "[',', 'because', 'the'] - Actual word:[duchess] vs Predicted word:[duchess]\n",
      "Iter= 29000, Average Loss= 0.014885, Average Accuracy= 99.80%\n",
      "[',', 'very', 'much'] - Actual word:[pleased] vs Predicted word:[pleased]\n",
      "Iter= 29500, Average Loss= 0.050282, Average Accuracy= 99.20%\n",
      "['much', 'like', 'keeping'] - Actual word:[so] vs Predicted word:[so]\n",
      "Iter= 30000, Average Loss= 0.087865, Average Accuracy= 98.80%\n",
      "['without', '\\xe2\\x80\\x94', 'maybe'] - Actual word:[it's] vs Predicted word:[it's]\n",
      "Iter= 30500, Average Loss= 0.080397, Average Accuracy= 98.20%\n",
      "['find', 'it', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 31000, Average Loss= 0.105491, Average Accuracy= 98.60%\n",
      "['a', 'very', 'hopeful'] - Actual word:[tone] vs Predicted word:[tone]\n",
      "Iter= 31500, Average Loss= 0.058859, Average Accuracy= 98.60%\n",
      "[\"'\", 'perhaps', 'it'] - Actual word:[hasn't] vs Predicted word:[hasn't]\n",
      "Iter= 32000, Average Loss= 0.041604, Average Accuracy= 99.80%\n",
      "['pepper', 'that', 'had'] - Actual word:[made] vs Predicted word:[made]\n",
      "Iter= 32500, Average Loss= 0.083304, Average Accuracy= 98.40%\n",
      "['of', 'that', 'is'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 33000, Average Loss= 0.037341, Average Accuracy= 99.40%\n",
      "['in', 'such', 'a'] - Actual word:[pleasant] vs Predicted word:[pleasant]\n",
      "Iter= 33500, Average Loss= 0.104515, Average Accuracy= 98.60%\n",
      "['my', 'dear', ','] - Actual word:[and] vs Predicted word:[and]\n",
      "Iter= 34000, Average Loss= 0.072933, Average Accuracy= 98.80%\n",
      "['she', 'tucked', 'her'] - Actual word:[arm] vs Predicted word:[arm]\n",
      "Iter= 34500, Average Loss= 0.077125, Average Accuracy= 98.20%\n",
      "['little', 'startled', 'when'] - Actual word:[she] vs Predicted word:[she]\n",
      "Iter= 35000, Average Loss= 0.075978, Average Accuracy= 98.60%\n",
      "['you', \"can't\", 'think'] - Actual word:[how] vs Predicted word:[how]\n",
      "Iter= 35500, Average Loss= 0.057382, Average Accuracy= 98.80%\n",
      "['people', 'knew', 'that'] - Actual word:[:] vs Predicted word:[:]\n",
      "Iter= 36000, Average Loss= 0.108955, Average Accuracy= 98.60%\n",
      "['her', 'chin', 'upon'] - Actual word:[alice's] vs Predicted word:[alice's]\n",
      "Iter= 36500, Average Loss= 0.021075, Average Accuracy= 99.80%\n",
      "['kind', 'of', 'rule'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 37000, Average Loss= 0.021038, Average Accuracy= 99.80%\n",
      "['keeping', 'so', 'close'] - Actual word:[to] vs Predicted word:[to]\n",
      "Iter= 37500, Average Loss= 0.199415, Average Accuracy= 97.20%\n",
      "['well', 'without', '\\xe2\\x80\\x94'] - Actual word:[maybe] vs Predicted word:[maybe]\n",
      "Iter= 38000, Average Loss= 0.047662, Average Accuracy= 99.40%\n",
      "['find', 'it', '.'] - Actual word:['] vs Predicted word:[']\n",
      "Iter= 38500, Average Loss= 0.178043, Average Accuracy= 97.60%\n",
      "['not', 'in', 'a'] - Actual word:[very] vs Predicted word:[very]\n",
      "Iter= 39000, Average Loss= 0.062909, Average Accuracy= 98.80%\n",
      "['remark', '.', \"'\"] - Actual word:[tut] vs Predicted word:[tut]\n",
      "Iter= 39500, Average Loss= 0.178403, Average Accuracy= 97.80%\n",
      "['pepper', 'that', 'had'] - Actual word:[made] vs Predicted word:[made]\n",
      "Iter= 40000, Average Loss= 0.085911, Average Accuracy= 98.20%\n",
      "['moral', 'of', 'that'] - Actual word:[is] vs Predicted word:[is]\n",
      "Iter= 40500, Average Loss= 0.098437, Average Accuracy= 98.20%\n",
      "['such', 'a', 'pleasant'] - Actual word:[temper] vs Predicted word:[temper]\n",
      "Iter= 41000, Average Loss= 0.035333, Average Accuracy= 99.00%\n",
      "['makes', 'you', 'forget'] - Actual word:[to] vs Predicted word:[to]\n",
      "Iter= 41500, Average Loss= 0.065189, Average Accuracy= 99.40%\n",
      "['together', '.', 'alice'] - Actual word:[was] vs Predicted word:[was]\n",
      "Iter= 42000, Average Loss= 0.054235, Average Accuracy= 99.00%\n",
      "['.', \"'\", \"you're\"] - Actual word:[thinking] vs Predicted word:[thinking]\n",
      "Iter= 42500, Average Loss= 0.044258, Average Accuracy= 99.40%\n",
      "['see', 'you', 'again'] - Actual word:[,] vs Predicted word:[,]\n",
      "Iter= 43000, Average Loss= 0.126153, Average Accuracy= 97.80%\n",
      "['so', 'stingy', 'about'] - Actual word:[it] vs Predicted word:[it]\n",
      "Iter= 43500, Average Loss= 0.083668, Average Accuracy= 98.20%\n",
      "['did', 'not', 'like'] - Actual word:[to] vs Predicted word:[to]\n",
      "Iter= 44000, Average Loss= 0.081818, Average Accuracy= 99.00%\n",
      "['and', '\\xe2\\x80\\x94', 'and'] - Actual word:[barley-sugar] vs Predicted word:[barley-sugar]\n",
      "Iter= 44500, Average Loss= 0.082923, Average Accuracy= 98.40%\n",
      "['right', 'height', 'to'] - Actual word:[rest] vs Predicted word:[rest]\n",
      "Iter= 45000, Average Loss= 0.109993, Average Accuracy= 97.60%\n",
      "['a', 'new', 'kind'] - Actual word:[of] vs Predicted word:[of]\n",
      "Iter= 45500, Average Loss= 0.031439, Average Accuracy= 99.00%\n",
      "['not', 'much', 'like'] - Actual word:[keeping] vs Predicted word:[keeping]\n",
      "Iter= 46000, Average Loss= 0.043662, Average Accuracy= 99.60%\n",
      "[\"it's\", 'always', 'pepper'] - Actual word:[that] vs Predicted word:[that]\n",
      "Iter= 46500, Average Loss= 0.076751, Average Accuracy= 99.20%\n",
      "['only', 'you', 'can'] - Actual word:[find] vs Predicted word:[find]\n",
      "Iter= 47000, Average Loss= 0.039539, Average Accuracy= 99.60%\n",
      "['hopeful', 'tone', 'though'] - Actual word:[)] vs Predicted word:[)]\n",
      "Iter= 47500, Average Loss= 0.061809, Average Accuracy= 99.20%\n",
      "[',', 'tut', ','] - Actual word:[child] vs Predicted word:[child]\n",
      "Iter= 48000, Average Loss= 0.049660, Average Accuracy= 99.20%\n",
      "['met', 'in', 'the'] - Actual word:[kitchen] vs Predicted word:[kitchen]\n",
      "Iter= 48500, Average Loss= 0.048818, Average Accuracy= 99.40%\n",
      "['i', 'shall', 'remember'] - Actual word:[it] vs Predicted word:[it]\n",
      "Iter= 49000, Average Loss= 0.096197, Average Accuracy= 98.80%\n",
      "['her', 'in', 'such'] - Actual word:[a] vs Predicted word:[a]\n",
      "Iter= 49500, Average Loss= 0.056252, Average Accuracy= 99.40%\n",
      "['that', 'makes', 'you'] - Actual word:[forget] vs Predicted word:[forget]\n",
      "Iter= 50000, Average Loss= 0.085314, Average Accuracy= 99.00%\n",
      "['affectionately', 'into', \"alice's\"] - Actual word:[,] vs Predicted word:[,]\n",
      "Optimization Finished!\n",
      "i only wish off together . alice was very glad to find her in such a pleasant temper , and thought to herself that perhaps it was only the pepper that\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 500\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip() for x in data]\n",
    "    data = [data[i].lower().split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    return data\n",
    "\n",
    "train_file = 'alice in wonderland.txt'\n",
    "train_data = read_data(train_file)\n",
    "\n",
    "\n",
    "def build_dataset(train_data):\n",
    "    count = collections.Counter(train_data).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(train_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "\n",
    "# Place holder for Mini batch input output\n",
    "x = tf.placeholder(\"float\", [None, n_input, 174])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}\n",
    "\n",
    "def input_one_hot(num):\n",
    "    x = np.zeros(vocab_size)\n",
    "    x[num] = 1 \n",
    "    return x.tolist()\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.unstack(x, n_input, 1)\n",
    "    print(np.shape(x))\n",
    "    ## 2 layered LSTM \n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but we only require the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    \n",
    "    while step < training_iters:\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[i])]) for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        \n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - Actual word:[%s] vs Predicted word:[%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Optimization Finished!\")\n",
    "   \n",
    "    sentence = 'i only wish'\n",
    "    words = sentence.split(' ')\n",
    "    try:\n",
    "        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[i])]) for i in range(offset, offset+n_input) ]\n",
    "        for i in range(28):\n",
    "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "            symbols_in_keys = symbols_in_keys[1:]\n",
    "            symbols_in_keys.append(input_one_hot(onehot_pred_index))\n",
    "        print(sentence)\n",
    "    except:\n",
    "        print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listing 4-5 . Text used as Corpus in Listing 4-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "' You can't think how glad I am to see you again , you dear old thing ! ' said the Duchess , as she tucked her arm affectionately into Alice's , and they walked off together . Alice was very glad to find her in such a pleasant temper , and thought to herself that perhaps it was only the pepper that had made her so savage when they met in the kitchen . ' When I'm a Duchess , ' she said to herself , ( not in a very hopeful tone though ) , ' I won't have any pepper in my kitchen at all . Soup does very well without — Maybe it's always pepper that makes people hot-tempered , ' she went on , very much pleased at having found out a new kind of rule , ' and vinegar that makes them sour — and camomile that makes them bitter — and — and barley-sugar and such things that make children sweet-tempered . I only wish people knew that : then they wouldn't be so stingy about it , you know — 'She had quite forgotten the Duchess by this time , and was a little startled when she heard her voice close to her\n",
    "\n",
    "ear . ' You're thinking about something , my dear , and that makes you forget to talk . I can't tell you just now what the moral of that is , but I shall remember it in a bit . ' ' Perhaps it hasn't one , ' Alice ventured to remark . ' Tut , tut , child ! ' said the Duchess . ' Everything's got a moral , if only you can find it . ' And she squeezed herself up closer to Alice's side as she spoke . Alice did not much like keeping so close to her : first , because the Duchess was very ugly ; and secondly , because she was exactly the right height to rest her chin upon Alice's shoulder , and it was an uncomfortably sharp chin . However, she did not like to be rude , so she bore it as well as she could . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code improved for Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "#training_iters = 50000\n",
    "training_iters = 2000\n",
    "display_step = 500\n",
    "n_input = 3\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        data = f.readlines()\n",
    "    data = [x.strip() for x in data]\n",
    "    data = [data[i].lower().split() for i in range(len(data))]\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, [-1, ])\n",
    "    return data\n",
    "\n",
    "train_file = 'alice in wonderland.txt'\n",
    "train_data = read_data(train_file)\n",
    "\n",
    "\n",
    "def build_dataset(train_data):\n",
    "    count = collections.Counter(train_data[0]).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(train_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "\n",
    "# Place holder for Mini batch input output\n",
    "#x = tf.placeholder(\"float\", [None, n_input, 174])\n",
    "x = tf.placeholder(\"float\", [None, n_input, vocab_size])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "}\n",
    "\n",
    "def input_one_hot(num):\n",
    "    x = np.zeros(vocab_size)\n",
    "    x[num] = 1 \n",
    "    return x.tolist()\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.unstack(x, n_input, 1)\n",
    "    print(np.shape(x))\n",
    "    ## 2 layered LSTM \n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but we only require the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "\n",
    "    \n",
    "    while step < training_iters:\n",
    "        if offset > (len(train_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "\n",
    "#        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[i])]) for i in range(offset, offset+n_input) ]\n",
    "#        print(range(offset, offset+n_input))\n",
    "#        print(train_data[0][0])\n",
    "#        print(train_data[0][1])\n",
    "#        print(train_data[0][2])\n",
    "\n",
    "        symbols_in_keys = [ train_data[0][i] for i in range(offset, offset+n_input) ]\n",
    "#        print(symbols_in_keys)\n",
    "\n",
    "        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[0][i])]) for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(train_data[0][offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "        _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        \n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                  \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [train_data[0][i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = train_data[0][offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - Actual word:[%s] vs Predicted word:[%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Optimization Finished!\")\n",
    "   \n",
    "    sentence = 'i only wish'\n",
    "    words = sentence.split(' ')\n",
    "    sentence_data = ['i','only','wish']\n",
    "#    symbols_in_keys = [ input_one_hot(dictionary[ str(sentence_data[i])]) for i in range(0, 3) ]\n",
    "#\n",
    "#    for i in range(28):\n",
    "#        keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "#        onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "#        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "#        sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "#        symbols_in_keys = symbols_in_keys[1:]\n",
    "#        symbols_in_keys.append(input_one_hot(onehot_pred_index))\n",
    "#    print(sentence)\n",
    "\n",
    "    \n",
    "    try:\n",
    "#        symbols_in_keys = [ input_one_hot(dictionary[ str(train_data[0][i])]) for i in range(offset, offset+n_input) ]\n",
    "        symbols_in_keys = [ input_one_hot(dictionary[ str(sentence_data[i])]) for i in range(0, 3) ]\n",
    "\n",
    "        for i in range(28):\n",
    "            keys = np.reshape(np.array(symbols_in_keys), [-1, n_input,vocab_size])\n",
    "            onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "            onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "            sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "            symbols_in_keys = symbols_in_keys[1:]\n",
    "            symbols_in_keys.append(input_one_hot(onehot_pred_index))\n",
    "        print(sentence)\n",
    "    except:\n",
    "        print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
